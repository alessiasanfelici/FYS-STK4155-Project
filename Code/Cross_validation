from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
import itertools

def cross_validation(X, z, k=(2, 11), method='OLS'):
    """
    Arguments:
    X: design matrix
    z: data
    k: number of folds  [start, stop[
    method : 'OLS', 'Ridge', 'Lasso'

    Returns:
    """
    degree = 8
    lambdas = np.logspace(-4, 6, 50)
    #nbfolds = k[1] - k[0]
    fig, (ax1, ax2) = plt.subplots(2, 1)

    
    values = np.linspace(0, 1, 10)
    cmap = plt.cm.get_cmap('viridis')
    colors = [cmap(x) for x in values]
    colors = itertools.cycle(colors)

    for i in reversed(range(k[0], k[1])):  # loop over number of folds Ã  l'envers
        if method == 'OLS':
            MSE_train = np.zeros((i, degree))
            MSE_test = np.zeros((i, degree))
        else:
            MSE_train = np.zeros((i, len(lambdas)))
            MSE_test = np.zeros((i, len(lambdas)))
        # shuffle indexes
        indices = np.arange(len(z))
        np.random.shuffle(indices)
        indices = np.array_split(indices, i)  # split index into k folds, without raising an error when the number of indexes is not divisible by k
        # for each fold, use the other k-1 folds for training and test on the remaining fold

        for j in range(len(indices)):  # loop over each fold
            # split data into training and test
            X_test = X[indices[j]]
            z_test = z[indices[j]]

            X_train = np.delete(X, indices[j], axis=0)
            z_train = np.delete(z, indices[j])

            # scale data (we only center the data here)
            X_train_mean = np.mean(X_train, axis=0)
            X_train = X_train - X_train_mean
            X_test = X_test - X_train_mean

            z_train_mean = np.mean(z_train)
            z_train = z_train - z_train_mean
            z_test = z_test - z_train_mean

            if method == 'OLS':
                for d in range(1, degree + 1):
                    c = int((d + 2) * (d + 1) / 2)
                    X_tilde = X_train[:, 0:c - 1]
                    beta = np.linalg.pinv(X_tilde.T @ X_tilde) @ X_tilde.T @ z_train

                    ypredict = X_tilde @ beta + z_train_mean
                    ypredict_test = X_test[:, 0:c - 1] @ beta + z_train_mean

                    MSE_train[j,d-1] = MSE(z_train, ypredict)
                    MSE_test[j,d-1] = MSE(z_test, ypredict_test)

            elif method == 'Ridge':
                degree = 5  # choose the degree of the polynomial

                for s, lmb in enumerate(lambdas):
                    ridge = Ridge(alpha=lmb)
                    ridge.fit(X_train, z_train)
                    z_train_predict = ridge.predict(X_train)
                    z_test_predict = ridge.predict(X_test)
                    MSE_train[j, s] = MSE(z_train, z_train_predict)
                    MSE_test[j, s] = MSE(z_test, z_test_predict)

            elif method == 'Lasso':
                degree = 5  # choose the degree of the polynomial

                for s, lmb in enumerate(lambdas):
                    lasso = Lasso(alpha=lmb)
                    lasso.fit(X_train, z_train)
                    z_train_predict = lasso.predict(X_train)
                    z_test_predict = lasso.predict(X_test)
                    MSE_train[j, s] = MSE(z_train, z_train_predict)
                    MSE_test[j, s] = MSE(z_test, z_test_predict)

        # return mean MSE (for each degree or lambda value)
        MSE_train_mean = np.mean(MSE_train, axis=0)
        MSE_test_mean = np.mean(MSE_test, axis=0)

        if i == k[1]-1:
            MSE_ref = MSE_test_mean

        MSE_diff = abs(MSE_test_mean - MSE_ref)

        color = next(colors)
        # plot MSE for each number of folds
        if method == 'OLS':
            degree_range = np.arange(1, degree + 1)
            #ax1.plot(degree_range, MSE_train_mean, color = color, label='k = %i' % i)
            ax1.plot(degree_range, MSE_test_mean, color = color, label ='k = %i' % i)
            ax2.plot(degree_range, MSE_diff, color = color, label ='k = %i' % i)

        else:
            #ax1.plot(lambdas, MSE_train_mean, color = color,  label='k = %i' % i)
            ax1.plot(lambdas, MSE_test_mean, color = color, label='k = %i' % i)
            ax2.plot(lambdas, MSE_diff, color = color, label='k = %i' % i)
        
        if method == 'OLS':
           ax1.set_xlabel('degree')
        else:
            ax1.set_xlabel('lambda')
        ax1.set_ylabel('MSE')
        ax1.set_title('MSE_test for different number of folds')

        
        ax1.legend(loc = 'upper left', fontsize = 8)
        ax2.legend(loc = 'upper left', fontsize = 8)
        ax2.set_title('MSE_test difference to  MSE_test for k = %i' % (k[1]-1), loc = 'center')

        if i == k[0]:
            MSE_ref = MSE_test_mean
    fig.subplots_adjust(hspace=0.5) 
    plt.show()

cross_validation(X, z)
cross_validation (X, z, method='Ridge')
cross_validation (X, z, method='Lasso')
